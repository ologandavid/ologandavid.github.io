<!DOCTYPE HTML>
<!--
	Solid State by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
	Modified by David Ologan for Personal Use
-->
<html>

<head>
	<title>David Ologan - Coursework</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
	<noscript>
		<link rel="stylesheet" href="assets/css/noscript.css" />
	</noscript>
</head>


<body class="is-preload">

	<!-- Page Wrapper -->
	<div id="page-wrapper">
		<header id="header" class="alt">
			<!--<h1 id="logo"><a href="index.html">Portfolio</a></h1>-->
			<nav>
				<a href="#menu">Menu</a>
			</nav>
			<nav id="menu">
				<div class="inner">
					<ul class="links">
						<li><a href="index.html">Home</a></li>
						<li><a href="Education.html">Education</a></li>
						<li><a href="Nav.html">Course Projects</a></li>
						<li><a href="Work Experience.html">Work Experience</a></li>
						<li><a href="Research.html">Research/ Projects</a></li>
						<li><a href="resume/David_Ologan_CV.pdf">Resume</a></li>
					</ul>
					<a href="#" class="close">Close</a>
			</nav>
	</div>
	</header>
	</header>

	<!-- Wrapper -->
	<section id="wrapper">
		<header>
			<div class="inner">
				<h2>Graduate Coursework</h2>
				<p>Capstone Projects, Labs, Writeups</p>
			</div>
		</header>

		<!-- Content -->
		<div class="wrapper">
			<div class="inner">
				<!-- 16.782 Begins Here-->
				<h3 class="major" id="s16.782"> 16.782 : Planning and Decision Making in Robotics (Fall 2023)</h3>
				<div class="box alt">
					<div class="row gtr-uniform">
						<div class="col-6"><span class="image fit"><img src="images/16.782/head2head.png"
									alt="" /></span>
						</div>
						<div class="col-5"><span class="vid fit"><video controls>
									<source src="images/16.782/head2head.mp4" type="video/mp4">
								</video></span></div>
					</div>
				</div>
				<p>This paper explores multi-robot motion planning strategies for quadruped robots. Here, we present
					three distinct methods: a sequential RRT-Connect (Rapidly Exploring Random Tree Planner), a joint
					state-space RRT-Connect Planner, and a Conflict Based Search. Our study showcases the success of
					each approach by generating kino-dynamically feasible and collision-free body trajectories for
					multiple legged robots navigating diverse terrain. Leveraging Quad-SDK, a open-source ROS-based
					framework designed for quadruped locomotion, we incorporate its footstep planner and low-level motor
					controller to implement these strategies. Applying all three global planner techniques proved
					successful in most scenarios with the conflict based planner approach having the lowest cost to
					goal. The sequential RRT had the quickest solve time due to its more simplistic approach.
				<p><b>Applied Skills: RRT, Conflict Based Search, C++, Multi-Robot Motion Planning</b>
				</p>
				<ul class="actions fit">
					<li><a href="https://github.com/ologandavid/multi-robot-quad-sdk"
							class="button primary fit">Github:</a>
					</li>
					<li><a href="PDFs/Multi_Robot_Motion_Planning_for_Quadruped_Robots.pdf" class="button fit">See the
							Complete Paper Here:</a></li>
				</ul>

				<!-- 16.745 Begins Here-->
				<h3 class="major" id="s16.745"> 16.745 : Optimal Control and Reinforcement Learning (Spring 2023)</h3>
				<div class="box alt">
					<div class="row gtr-uniform">
						<div class="col-6"><span class="image fit"><img src="images/16.745/tmp.gif" alt="" /></span>
						</div>
						<div class="col-6"><span class="vid fit"><video controls>
									<source src="images/16.745/meshcat1.mp4" type="video/mp4">
								</video></span></div>
					</div>
				</div>
				<p>In this paper we show a trajectory planning technique that mimics a monkey bar robot swinging from
					bar to bar.
					Using a hybrid system direct collocation (DIRCOL) trajectory optimization, we successfully
					demonstrate the robot swinging
					up from a dead hang to catch the first bar and swing to the subsequent bars. This DIRCOL technique
					was tested on various
					mass distributions in the robot as well as different bar separation distances to understand the
					behavior with varying parameters.
					In addition, we show the importance of a free time setup on the cost function in producing
					consistent feasible trajectories using
					this DIRCOL technique.
				<p><b>Applied Skills: DIRCOL, Hybrid Systems, Lagrangian Dynamics, iPOPT Trajectory Optimization</b>
				</p>
				<ul class="actions fit">
					<li><a href="https://github.com/ologandavid/MonkeyBarBot" class="button primary fit">Github:</a>
					</li>
					<li><a href="PDFs/MonkeyBot.pdf" class="button fit">See the Complete Paper Here:</a></li>
				</ul>

				<!-- 11.785 Begins Here-->
				<h3 class="major" id="s11.785"> 11.785 : Introduction to Deep Learning (Spring 2023)</h3>
				<div class="box alt">
					<div class="row gtr-uniform">
						<div class="col-6"><span class="image fit"><img src="images/11.785/Picture1.png"
									alt="" /></span></div>
						<div class="col-6"><span class="image fit"><img src="images/11.785/xukungif.gif"
									alt="" /></span></div>
					</div>
				</div>
				<p>In recent years, there has been a large research focus on dense video captioning.
					Video captioning has applications in many fields such as autonomous driving,
					video surveillance and creating captions for those with visual impairments. As
					such, a novel approach is proposed by employing language models (LM) to help to
					semantically align video features in an attempt to improve the overall performance
					of dense video captioning. The baseline model for comparison, End-to-end dense
					video captioning with parallel decoding (PDVC) [1], produced strong results
					compared to many state of the art video captioning frameworks. PDVC is trained
					on the ActivityNet and YouCook2 datasets but for this study only YouCook2
					was used due to computational capacity limits. We propose the incorporation
					of semantic alignment through the addition of a tuner network before the video
					features are passed through the PDVC framework. Ablations were ran for different
					tuner architectures and overall, the modified PDVC framework outperformed the
					baseline PDVC in many evaluation metrics. Promising future extensions with
					Semantic alignment and Dense Video Captioning remain with its application to
					larger and more comprehensive data sets.
					<br>
				<p><b>Applied Skills: PyTorch, Linux, GCP, Python, Deep Learning</b>
				</p>
				<ul class="actions fit">
					<li><a href="https://github.com/ologandavid/DenseVideoCaptioning"
							class="button primary fit">Github:</a></li>
					<li><a href="PDFs/11_785_Final_Project.pdf" class="button fit">See the Complete Paper Here:</a></li>
				</ul>

				<!-- 24.678 Begins Here-->
				<h3 class="major" id="s24.678"> 24.678 : Computer Vision for Engineers (Fall 2022)</h3>
				<p><b>Objective:</b><br>
					While taking CMU’s 24.678, my team and I decided to apply traditional computer vision techniques to
					tackle the growing incidence rates of fatal injuries on construction sites nationwide. As the number of
					such injuries has risen by nearly 90% in the last three years, my team devised a method to improve
					worker awareness and safety on jobsites, maximizing company profits and reducing worker downtime.<br><br>
					<b>Solution:</b><br>
					My team developed a system that combines image processing and point cloud-based computer vision
					methods to track the movement of workers around construction sites and notify them when they enter
					potentially dangerous regions. For simplicity, we targeted the most common construction site hazard,
					workers being struck by falling objects. Ideally, we’d like this technology to allow us to prevent
					some of the accidents we see every year.<br><br>
					To solve this problem, we propose a multistep method. First, we preprocess our worksite by
					selecting four fixed reference points in the camera’s field of view. Given a video stream of the
					worksite, we need a reliable way to make worker positions in the image to worker positions in space.
					To do this, we operate under the assumption that the camera position is fixed, and all workers are
					on the same plane (the ground). This allows us to generate a static transformation that maps pixel
					values to world coordinate values (assuming the worksite shape and dimensions are known). This allows
					us to account for the unique perspective of our camera and generate the respective transformation
					matrix.
					<div class="box alt">
						<div class="row gtr-uniform">
							<div class="col-3"></div>
							<div class="col-6"><span class="image fit"><img src="images/24.678/network.png"
										alt="" /></span></div>
							<div class="col-3"></div>
						</div>
					</div>
					Subsequently, we segment our video stream into individual frames that our YOLOv7 network can
					process. Transfer learning was done to refine the performance of the network on our construction
					site dataset. Using YOLO for object detection, we can identify workers in each image, and output a
					corresponding object bounding box. Given the pixel values for the center of the bounding box, we can
					extract the workers position by average the bottom of the box.
					<div class="box alt">
						<div class="row gtr-uniform">
							<div class="col-3"></div>
							<div class="col-6"><span class="image fit"><img src="images/24.678/Picture2.png"
										alt="" /></span></div>
							<div class="col-3"></div>
						</div>
					</div>
					Using our predefined transformation matrix, we can map video coordinates to world coordinates, and
					generate a map of worker positions for each frame. These coordinates are also scaled by the size our
					output frame and boundary. Each frame is then stitched together to create an animation that
					describes worker position progression over time. For our simulation, we predefine danger areas,
					although a network can be trained to identify risky or potentially dangerous objects.<br> <br>

					<b>Results:</b><br>
					For a quick representation of worker position, we plotted their location from a birds-eye view, In
					the future, we plan to generate a more comprehensive representation of worker position, perhaps
					incorporating velocities or making predictions about future worker behavior. Extending the method to
					work with multiple cameras and occlusions also remains future work. Ultimately, we were able to
					successfully track the location of each worker in a variety of environments.

				<div class="box alt">
					<div class="row gtr-uniform">
						<div class="col-6"><span class="vid fit"><video width="640" height="480" controls>
									<source src="images/24.678/test4.mp4" type="video/mp4">
								</video></span></div>
						<div class="col-6"><span class="vid fit"><video width="640" height="480" controls>
									<source src="images/24.678/vid3.mp4" type="video/mp4">
								</video></span></div>
					</div>
				</div>
				</p>
				<p><b>Applied Skills: OpenCV, Python, Coordinate Transformations, Object Detection, Transfer
						Learning</b>
				</p>
				<ul class="actions fit">
					<li><a href="https://github.com/ologandavid/ConstructionSiteHazardDetection"
							class="button primary fit">Github:</a></li>
					<li><a href="PDFs/FinalPresentation.pptx" class="button fit">See the Presentation Here:</a></li>
				</ul>

				<!-- 24.695 Begins Here-->
				<h3 class="major" id="s24.695"> 24.695 : Modern Control Theory (Fall 2022)</h3>
				<p><b>Objective:</b><br>
					As a student in 24.695, CMU’s Control Theory class, I was assigned the challenge of creating an
					efficient controller and estimator for a small self-driving vehicle. Given the projected expansion
					of the driverless car industry to reach 93 billion by 2028, this project is geared towards
					addressing that growth. For this project, we drew inspiration from <a
						href="https://www.cmu.edu/buggy/">CMU’s yearly buggy competition</a>
					for our track design and developed an optimal controller for the car.<br><br>
					<b>Solution:</b><br> <span class="image right"><img src="images/24.695/bicycle.png" alt="" /></span>
					To approximate the motion of the car, a simple bicycle model was used to define system dynamics. The
					car is modeled as a two wheeled vehicle with two degrees of freedom described by its longitudinal
					and lateral dynamics. As such, I designed a two-part controller that generates control commands
					including desired steering angle δ and longitudinal force f.
					Given a desired trajectory of waypoints, I implemented a:
				<ol>
					<li>PID Controller</li>
					<li>State Feedback Controller</li>
					<li>LQR Controller</li>
					<li>MPC Controller</li>
				</ol>
				To optimize the performance of the PID controller, I tuned the associated proportional, derivative,
				and integral gains. For the static LQR controller, I discretized the continuous error dynamics of
				the nonlinear system using a zero order hold, and then designed an infinite horizon LQR controller
				for the system. Similarly, to implement the MPC, I implemented an iterative finite horizon LQR for a
				tunable horizon size N in the future. Ultimately, the controller was able to get the buggy to
				accelerate on the straight portions of the track, while slowing accordingly for turns.<br>
				<div class="box alt">
					<div class="row gtr-uniform">
						<div class="col-5"><span class="image fit"><img src="images/24.695/track.png" alt="" /></span>
						</div>
						<div class="col-6"><span class="vid fit"><video controls>
									<source src="images/24.695/1080.mp4" type="video/mp4">
								</video></span></div>
					</div>
				</div>
				In the event that localization information is missing from GPS, autonomous cars often use CV/LIDAR
				based methods for obstacle avoidance. In an effort to incorporate this into our model, I implemented
				EKF SLAM, a sensor fusion method used to augment our position estimate with local measurements.
				Taking advantage range and bearing measurements to the next waypoint, the buggy was able to maintain
				a stable estimate of position and heading over the entire track.<br><br>

				<b>Results:</b><br>
				To assess the performance of our simulation, we tested the model on a track modeled after CMU’s
				buggy course. Driving simulations were then performed using Webots software. The model was able to
				complete the track in under 120 seconds and had an average deviation of less than 3 meters from the
				optimal tracked path.
				<div class="box alt">
					<div class="row gtr-uniform">
						<div class="col-6"><span class="image fit"><img src="images/24.695/LQR_Figure_1.png"
									alt="" /></span></div>
						<div class="col-5"><span class="image fit"><img src="images/24.695/Figure_1.png"
									alt="" /></span></div>
					</div>
				</div>
				This performance was substantially better than the real-world buggy raced every year at CMU. By the
				end of the course, each controller design was able to clear desired performance evaluations.
				</p>
				<p><b>Applied Skills: Python, Webots, Extended Kalman Filter SLAM, MPC, LQR, State Feedback, PID </b>
				</p>
			</div>
		</div>
	</section>

	<!-- Footer -->
	<section id="footer">
		<div class="inner">
			<h2 class="major">Contact</h2>
			<p>If you have any questions, please feel free to contact me at:</p>
			<ul class="contact">
				<!--<li class="icon solid fa-phone"><a href="#">(917) 635-2734 </a> </li>-->
				<li class="icon solid fa-envelope"><a href="#">ologandavid@gmail.com </a></li>
				<li class="icon brands fa-linkedin"><a href="https://linkedin.com/in/dologan">
						www.linkedin.com/in/dologan </a></li>
				<li class="icon brands fa-github"><a href="https://github.com/ologandavid"> www.github.com/ologandavid
					</a></li>
			</ul>
			<ul class="copyright">
				<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
			</ul>
		</div>
	</section>
	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>